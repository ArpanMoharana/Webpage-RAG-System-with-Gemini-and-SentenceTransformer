# -*- coding: utf-8 -*-
"""Webpage-RAG-System-with-Gemini-and-SentenceTransformer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rErbBzn7Rm8udvXb_qJw1bT5S3Tnp2Pk
"""

!pip install -U google-generativeai
!pip install langchain langchain-community langchain-google-genai sentence-transformers chromadb langchain-text-splitters

import os
import getpass

# Secure input box (hides your API key)
os.environ["GOOGLE_API_KEY"] = getpass.getpass("üîë Enter your Gemini API key and press Enter Key: ")

# Confirm setup (without showing the key)
if os.environ["GOOGLE_API_KEY"]:
    print("‚úÖ Gemini API Key set successfully (not displayed for security).")
else:
    print("‚ùå No key was entered. Please try again.")

import google.generativeai as genai  #testing the model

genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
model = genai.GenerativeModel("gemini-2.5-flash")

print(model.generate_content("Hello! Are you working?").text)

from langchain_community.document_loaders import WebBaseLoader #Load Documents (from the Web)

loader = WebBaseLoader("https://flatsixes.com/porsche-news/porsche-sets-november-19-world-debut-for-cayenne-electric/")
docs = loader.load()
docs

from langchain_text_splitters import RecursiveCharacterTextSplitter #Split Documents into Chunks

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=3000,
    chunk_overlap=300
)

splits = text_splitter.split_documents(docs)
len(splits)

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

embeddings = HuggingFaceEmbeddings(         #Convert Chunks to Embeddings (Local ‚Äî No API Needed) , no rate limit issue using HuggingFace
    model_name="all-MiniLM-L6-v2",
    model_kwargs={"device": "cpu"}          #cpu instead of gpu as best for free google colab version and headache free
)

vectordb = Chroma.from_documents(
    splits,
    embedding=embeddings,
    persist_directory="chroma_store"
)

vectordb.persist()                   #‚ÄúThis code stores the chunked documents and their embeddings in a persistent Chroma vector database,
                                      #allowing fast semantic search for the RAG system.‚Äù

vectordb._collection.count()

retriever = vectordb.as_retriever( #This line converts your Chroma Vector Database into a Retriever object that LangChain can use during RAG.
    search_type="similarity",
    search_kwargs={"k": 3}       #Return the top 3 most relevant chunks for every question.
)

from langchain_google_genai import ChatGoogleGenerativeAI        #This imports the Gemini LLM wrapper for LangChain

llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
)

from langchain.chains import RetrievalQA

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff"
)
# Create the RetrievalQA RAG pipeline:
# - Uses our Gemini LLM to generate answers
# - Uses the retriever to fetch the top-k most relevant document chunks
# - chain_type="stuff" means all retrieved chunks are combined into a single prompt
# This forms the complete RAG system (Retrieve ‚Üí Inject Context ‚Üí Answer)

response = qa_chain.invoke({"query": "When is the Cayenne Electric's world debut?"}) #q1
print(response["result"])

qa_chain.invoke({"query": "Summarize the article in 5 bullet points."})["result"] #q2

response = qa_chain.invoke({"query": "What Model Of Porche are we talking about?"}) #q3
print(response["result"])